{"loss": 1.47289829, "grad_norm": 1.82907903, "learning_rate": 2.893e-05, "token_acc": 0.61957961, "epoch": 1.95097058, "global_step/max_steps": "610/939", "percentage": "64.96%", "elapsed_time": "3m 27s", "remaining_time": "1m 51s", "memory(GiB)": 22.9, "train_speed(iter/s)": 2.939763}
{"loss": 1.48590717, "grad_norm": 1.97908282, "learning_rate": 2.738e-05, "token_acc": 0.61559373, "epoch": 1.98298979, "global_step/max_steps": "620/939", "percentage": "66.03%", "elapsed_time": "6m 49s", "remaining_time": "3m 30s", "memory(GiB)": 22.9, "train_speed(iter/s)": 1.514246}
{"loss": 1.46650019, "grad_norm": 1.92756236, "learning_rate": 2.585e-05, "token_acc": 0.62055147, "epoch": 2.01280768, "global_step/max_steps": "630/939", "percentage": "67.09%", "elapsed_time": "10m 2s", "remaining_time": "4m 55s", "memory(GiB)": 22.9, "train_speed(iter/s)": 1.046357}
{"loss": 1.5217659, "grad_norm": 1.91895711, "learning_rate": 2.436e-05, "token_acc": 0.61528527, "epoch": 2.0448269, "global_step/max_steps": "640/939", "percentage": "68.16%", "elapsed_time": "13m 24s", "remaining_time": "6m 15s", "memory(GiB)": 22.9, "train_speed(iter/s)": 0.795573}
{"loss": 1.51918058, "grad_norm": 1.96035182, "learning_rate": 2.289e-05, "token_acc": 0.61843133, "epoch": 2.07684611, "global_step/max_steps": "650/939", "percentage": "69.22%", "elapsed_time": "16m 47s", "remaining_time": "7m 28s", "memory(GiB)": 22.9, "train_speed(iter/s)": 0.644988}
{"loss": 1.50009737, "grad_norm": 1.95691431, "learning_rate": 2.145e-05, "token_acc": 0.61804683, "epoch": 2.10886532, "global_step/max_steps": "660/939", "percentage": "70.29%", "elapsed_time": "20m 11s", "remaining_time": "8m 32s", "memory(GiB)": 22.9, "train_speed(iter/s)": 0.544862}
{"loss": 1.5209671, "grad_norm": 1.8806138, "learning_rate": 2.005e-05, "token_acc": 0.61414791, "epoch": 2.14088453, "global_step/max_steps": "670/939", "percentage": "71.35%", "elapsed_time": "23m 35s", "remaining_time": "9m 28s", "memory(GiB)": 22.9, "train_speed(iter/s)": 0.473469}
{"loss": 1.50629883, "grad_norm": 2.16684842, "learning_rate": 1.869e-05, "token_acc": 0.62138465, "epoch": 2.17290374, "global_step/max_steps": "680/939", "percentage": "72.42%", "elapsed_time": "26m 59s", "remaining_time": "10m 16s", "memory(GiB)": 22.9, "train_speed(iter/s)": 0.420011}
{"loss": 1.46650372, "grad_norm": 2.42181087, "learning_rate": 1.736e-05, "token_acc": 0.62146475, "epoch": 2.20492295, "global_step/max_steps": "690/939", "percentage": "73.48%", "elapsed_time": "30m 23s", "remaining_time": "10m 57s", "memory(GiB)": 22.9, "train_speed(iter/s)": 0.37844}
{"loss": 1.51116619, "grad_norm": 2.0692687, "learning_rate": 1.608e-05, "token_acc": 0.61474455, "epoch": 2.23694217, "global_step/max_steps": "700/939", "percentage": "74.55%", "elapsed_time": "33m 47s", "remaining_time": "11m 32s", "memory(GiB)": 22.9, "train_speed(iter/s)": 0.345279}
{"loss": 1.48757362, "grad_norm": 1.96778059, "learning_rate": 1.483e-05, "token_acc": 0.6213464, "epoch": 2.26896138, "global_step/max_steps": "710/939", "percentage": "75.61%", "elapsed_time": "37m 12s", "remaining_time": "12m 0s", "memory(GiB)": 22.9, "train_speed(iter/s)": 0.318022}
{"loss": 1.46056499, "grad_norm": 2.04445934, "learning_rate": 1.362e-05, "token_acc": 0.62885389, "epoch": 2.30098059, "global_step/max_steps": "720/939", "percentage": "76.68%", "elapsed_time": "40m 36s", "remaining_time": "12m 20s", "memory(GiB)": 22.9, "train_speed(iter/s)": 0.295558}
{"loss": 1.46994076, "grad_norm": 2.05163527, "learning_rate": 1.246e-05, "token_acc": 0.62591474, "epoch": 2.3329998, "global_step/max_steps": "730/939", "percentage": "77.74%", "elapsed_time": "43m 57s", "remaining_time": "12m 35s", "memory(GiB)": 22.9, "train_speed(iter/s)": 0.276731}
{"loss": 1.49091396, "grad_norm": 2.28560853, "learning_rate": 1.134e-05, "token_acc": 0.62054756, "epoch": 2.36501901, "global_step/max_steps": "740/939", "percentage": "78.81%", "elapsed_time": "47m 21s", "remaining_time": "12m 44s", "memory(GiB)": 22.9, "train_speed(iter/s)": 0.260464}
{"loss": 1.4982399, "grad_norm": 2.33519125, "learning_rate": 1.027e-05, "token_acc": 0.61893505, "epoch": 2.39703822, "global_step/max_steps": "750/939", "percentage": "79.87%", "elapsed_time": "50m 43s", "remaining_time": "12m 46s", "memory(GiB)": 22.9, "train_speed(iter/s)": 0.246438}
{"loss": 1.5113862, "grad_norm": 2.01956725, "learning_rate": 9.25e-06, "token_acc": 0.61156069, "epoch": 2.42905743, "global_step/max_steps": "760/939", "percentage": "80.94%", "elapsed_time": "54m 8s", "remaining_time": "12m 44s", "memory(GiB)": 22.9, "train_speed(iter/s)": 0.23399}
{"loss": 1.50823536, "grad_norm": 1.96260679, "learning_rate": 8.27e-06, "token_acc": 0.61860727, "epoch": 2.46107665, "global_step/max_steps": "770/939", "percentage": "82.00%", "elapsed_time": "57m 31s", "remaining_time": "12m 37s", "memory(GiB)": 22.9, "train_speed(iter/s)": 0.223088}
{"loss": 1.46817789, "grad_norm": 2.05655646, "learning_rate": 7.35e-06, "token_acc": 0.62440686, "epoch": 2.49309586, "global_step/max_steps": "780/939", "percentage": "83.07%", "elapsed_time": "1h 0m 54s", "remaining_time": "12m 24s", "memory(GiB)": 22.9, "train_speed(iter/s)": 0.213455}
{"loss": 1.5033247, "grad_norm": 2.15229368, "learning_rate": 6.47e-06, "token_acc": 0.6173044, "epoch": 2.52511507, "global_step/max_steps": "790/939", "percentage": "84.13%", "elapsed_time": "1h 4m 16s", "remaining_time": "12m 7s", "memory(GiB)": 22.9, "train_speed(iter/s)": 0.204837}
{"loss": 1.44175034, "grad_norm": 2.03514242, "learning_rate": 5.65e-06, "token_acc": 0.62893843, "epoch": 2.55713428, "global_step/max_steps": "800/939", "percentage": "85.20%", "elapsed_time": "1h 7m 39s", "remaining_time": "11m 45s", "memory(GiB)": 22.9, "train_speed(iter/s)": 0.197087}
{"loss": 1.48128328, "grad_norm": 2.172647, "learning_rate": 4.88e-06, "token_acc": 0.61947359, "epoch": 2.58915349, "global_step/max_steps": "810/939", "percentage": "86.26%", "elapsed_time": "1h 11m 3s", "remaining_time": "11m 18s", "memory(GiB)": 22.9, "train_speed(iter/s)": 0.189999}
{"loss": 1.50307989, "grad_norm": 2.02893043, "learning_rate": 4.16e-06, "token_acc": 0.61448119, "epoch": 2.6211727, "global_step/max_steps": "820/939", "percentage": "87.33%", "elapsed_time": "1h 14m 26s", "remaining_time": "10m 48s", "memory(GiB)": 22.9, "train_speed(iter/s)": 0.183574}
{"loss": 1.48016291, "grad_norm": 2.35845232, "learning_rate": 3.5e-06, "token_acc": 0.6190503, "epoch": 2.65319192, "global_step/max_steps": "830/939", "percentage": "88.39%", "elapsed_time": "1h 17m 50s", "remaining_time": "10m 13s", "memory(GiB)": 22.9, "train_speed(iter/s)": 0.177722}
{"loss": 1.49810314, "grad_norm": 2.11852503, "learning_rate": 2.89e-06, "token_acc": 0.62217866, "epoch": 2.68521113, "global_step/max_steps": "840/939", "percentage": "89.46%", "elapsed_time": "1h 21m 13s", "remaining_time": "9m 34s", "memory(GiB)": 22.9, "train_speed(iter/s)": 0.172366}
{"loss": 1.45548477, "grad_norm": 2.01620483, "learning_rate": 2.34e-06, "token_acc": 0.62401586, "epoch": 2.71723034, "global_step/max_steps": "850/939", "percentage": "90.52%", "elapsed_time": "1h 24m 35s", "remaining_time": "8m 51s", "memory(GiB)": 22.9, "train_speed(iter/s)": 0.16746}
{"loss": 1.47332211, "grad_norm": 2.16092134, "learning_rate": 1.85e-06, "token_acc": 0.62285165, "epoch": 2.74924955, "global_step/max_steps": "860/939", "percentage": "91.59%", "elapsed_time": "1h 27m 58s", "remaining_time": "8m 4s", "memory(GiB)": 22.9, "train_speed(iter/s)": 0.162933}
{"loss": 1.48723707, "grad_norm": 2.13245678, "learning_rate": 1.41e-06, "token_acc": 0.61828539, "epoch": 2.78126876, "global_step/max_steps": "870/939", "percentage": "92.65%", "elapsed_time": "1h 31m 22s", "remaining_time": "7m 14s", "memory(GiB)": 22.9, "train_speed(iter/s)": 0.158697}
{"loss": 1.4924695, "grad_norm": 2.2992053, "learning_rate": 1.03e-06, "token_acc": 0.61255984, "epoch": 2.81328797, "global_step/max_steps": "880/939", "percentage": "93.72%", "elapsed_time": "1h 34m 45s", "remaining_time": "6m 21s", "memory(GiB)": 22.9, "train_speed(iter/s)": 0.154769}
{"loss": 1.5368185, "grad_norm": 2.10961223, "learning_rate": 7.1e-07, "token_acc": 0.60843959, "epoch": 2.84530718, "global_step/max_steps": "890/939", "percentage": "94.78%", "elapsed_time": "1h 38m 9s", "remaining_time": "5m 24s", "memory(GiB)": 22.9, "train_speed(iter/s)": 0.151106}
{"loss": 1.45652065, "grad_norm": 2.19874072, "learning_rate": 4.5e-07, "token_acc": 0.62301342, "epoch": 2.8773264, "global_step/max_steps": "900/939", "percentage": "95.85%", "elapsed_time": "1h 41m 34s", "remaining_time": "4m 24s", "memory(GiB)": 22.9, "train_speed(iter/s)": 0.147686}
{"loss": 1.52574005, "grad_norm": 2.01742029, "learning_rate": 2.5e-07, "token_acc": 0.6147927, "epoch": 2.90934561, "global_step/max_steps": "910/939", "percentage": "96.91%", "elapsed_time": "1h 44m 59s", "remaining_time": "3m 20s", "memory(GiB)": 22.9, "train_speed(iter/s)": 0.144467}
{"loss": 1.46338167, "grad_norm": 2.09211159, "learning_rate": 1.1e-07, "token_acc": 0.62379421, "epoch": 2.94136482, "global_step/max_steps": "920/939", "percentage": "97.98%", "elapsed_time": "1h 48m 22s", "remaining_time": "2m 14s", "memory(GiB)": 22.9, "train_speed(iter/s)": 0.141478}
{"loss": 1.45669079, "grad_norm": 2.16746426, "learning_rate": 2e-08, "token_acc": 0.62313325, "epoch": 2.97338403, "global_step/max_steps": "930/939", "percentage": "99.04%", "elapsed_time": "1h 51m 46s", "remaining_time": "1m 4s", "memory(GiB)": 22.9, "train_speed(iter/s)": 0.13867}
{"train_runtime": 6879.4262, "train_samples_per_second": 2.179, "train_steps_per_second": 0.136, "total_flos": 9.702351027248886e+17, "train_loss": 0.53715092, "token_acc": 0.62095775, "epoch": 3.0, "global_step/max_steps": "939/939", "percentage": "100.00%", "elapsed_time": "1h 54m 36s", "remaining_time": "0s", "memory(GiB)": 22.9, "train_speed(iter/s)": 0.136561}
{"model_parameter_info": "PeftModelForCausalLM: 8332.5368M Params (40.3702M Trainable [0.4845%]), 0.0019M Buffers.", "last_model_checkpoint": "/data/xhsGPT/checkpoints/qwen2.5_vl_xhs/v4-20251130-163010/checkpoint-939", "best_model_checkpoint": null, "best_metric": null, "global_step": 939, "log_history": [{"epoch": 0.003201921152691615, "grad_norm": 0.9427400827407837, "learning_rate": 3.448275862068966e-06, "loss": 3.034529447555542, "step": 1, "token_acc": 0.4174095437860514}, {"epoch": 0.03201921152691615, "grad_norm": 1.0394667387008667, "learning_rate": 3.4482758620689657e-05, "loss": 2.9798384772406683, "step": 10, "token_acc": 0.42581638940234134}, {"epoch": 0.0640384230538323, "grad_norm": 1.0350247621536255, "learning_rate": 6.896551724137931e-05, "loss": 2.6860893249511717, "step": 20, "token_acc": 0.44986680269795387}, {"epoch": 0.09605763458074845, "grad_norm": 0.9171334505081177, "learning_rate": 9.999970204097939e-05, "loss": 2.3804121017456055, "step": 30, "token_acc": 0.48846326164874554}, {"epoch": 0.1280768461076646, "grad_norm": 0.8646963238716125, "learning_rate": 9.996395125523828e-05, "loss": 2.23023681640625, "step": 40, "token_acc": 0.5047477907678797}, {"epoch": 0.16009605763458074, "grad_norm": 0.8422906994819641, "learning_rate": 9.986865748457457e-05, "loss": 2.0958608627319335, "step": 50, "token_acc": 0.5233090354297338}, {"epoch": 0.1921152691614969, "grad_norm": 0.9719605445861816, "learning_rate": 9.971393429237563e-05, "loss": 2.091337203979492, "step": 60, "token_acc": 0.5250231696014829}, {"epoch": 0.22413448068841305, "grad_norm": 1.0380438566207886, "learning_rate": 9.949996606519354e-05, "loss": 2.0668109893798827, "step": 70, "token_acc": 0.5281933899062081}, {"epoch": 0.2561536922153292, "grad_norm": 1.0209375619888306, "learning_rate": 9.922700779300817e-05, "loss": 1.993912124633789, "step": 80, "token_acc": 0.5388459791004089}, {"epoch": 0.28817290374224536, "grad_norm": 1.1782763004302979, "learning_rate": 9.889538476535084e-05, "loss": 2.0172470092773436, "step": 90, "token_acc": 0.5396191996351614}, {"epoch": 0.32019211526916147, "grad_norm": 1.4765483140945435, "learning_rate": 9.850549218365053e-05, "loss": 2.037888526916504, "step": 100, "token_acc": 0.5312882752477452}, {"epoch": 0.35221132679607764, "grad_norm": 1.3002610206604004, "learning_rate": 9.805779469026501e-05, "loss": 1.8951194763183594, "step": 110, "token_acc": 0.5468239876281574}, {"epoch": 0.3842305383229938, "grad_norm": 1.3413561582565308, "learning_rate": 9.755282581475769e-05, "loss": 1.8798851013183593, "step": 120, "token_acc": 0.557489901575923}, {"epoch": 0.4162497498499099, "grad_norm": 1.4786187410354614, "learning_rate": 9.69911873380803e-05, "loss": 1.901313591003418, "step": 130, "token_acc": 0.5494579945799458}, {"epoch": 0.4482689613768261, "grad_norm": 1.3467682600021362, "learning_rate": 9.63735485754193e-05, "loss": 1.8468931198120118, "step": 140, "token_acc": 0.559231468849477}, {"epoch": 0.48028817290374226, "grad_norm": 1.4892834424972534, "learning_rate": 9.570064557856017e-05, "loss": 1.8889873504638672, "step": 150, "token_acc": 0.5515770051317535}, {"epoch": 0.5123073844306584, "grad_norm": 1.4015499353408813, "learning_rate": 9.49732802587207e-05, "loss": 1.9102113723754883, "step": 160, "token_acc": 0.5493293426449396}, {"epoch": 0.5443265959575746, "grad_norm": 1.4867732524871826, "learning_rate": 9.419231943089822e-05, "loss": 1.8641101837158203, "step": 170, "token_acc": 0.5589955280357757}, {"epoch": 0.5763458074844907, "grad_norm": 1.3704677820205688, "learning_rate": 9.335869378086972e-05, "loss": 1.8484542846679688, "step": 180, "token_acc": 0.5616201117318436}, {"epoch": 0.6083650190114068, "grad_norm": 1.5321389436721802, "learning_rate": 9.247339675607605e-05, "loss": 1.8850658416748047, "step": 190, "token_acc": 0.5565334379729805}, {"epoch": 0.6403842305383229, "grad_norm": 1.4659943580627441, "learning_rate": 9.153748338171192e-05, "loss": 1.8297218322753905, "step": 200, "token_acc": 0.5576579593659299}, {"epoch": 0.6724034420652392, "grad_norm": 1.466857671737671, "learning_rate": 9.055206900343228e-05, "loss": 1.797056007385254, "step": 210, "token_acc": 0.5657085337210603}, {"epoch": 0.7044226535921553, "grad_norm": 1.484342336654663, "learning_rate": 8.951832795817397e-05, "loss": 1.863274383544922, "step": 220, "token_acc": 0.5533819058000115}, {"epoch": 0.7364418651190714, "grad_norm": 1.7169773578643799, "learning_rate": 8.843749217467612e-05, "loss": 1.8710081100463867, "step": 230, "token_acc": 0.5552533718155076}, {"epoch": 0.7684610766459876, "grad_norm": 1.7191188335418701, "learning_rate": 8.731084970536738e-05, "loss": 1.864406204223633, "step": 240, "token_acc": 0.555311194193974}, {"epoch": 0.8004802881729037, "grad_norm": 1.534696340560913, "learning_rate": 8.613974319136958e-05, "loss": 1.7848968505859375, "step": 250, "token_acc": 0.5657955330416763}, {"epoch": 0.8324994996998198, "grad_norm": 1.367798924446106, "learning_rate": 8.492556826244687e-05, "loss": 1.7934467315673828, "step": 260, "token_acc": 0.5648309613545592}, {"epoch": 0.8645187112267361, "grad_norm": 1.6910395622253418, "learning_rate": 8.36697718738073e-05, "loss": 1.8091894149780274, "step": 270, "token_acc": 0.5669140800719749}, {"epoch": 0.8965379227536522, "grad_norm": 1.4645434617996216, "learning_rate": 8.237385058173912e-05, "loss": 1.8422256469726563, "step": 280, "token_acc": 0.5582601755786113}, {"epoch": 0.9285571342805683, "grad_norm": 1.568582534790039, "learning_rate": 8.103934876013623e-05, "loss": 1.7899974822998046, "step": 290, "token_acc": 0.5666529096161783}, {"epoch": 0.9605763458074845, "grad_norm": 1.4879812002182007, "learning_rate": 7.966785676003866e-05, "loss": 1.7816152572631836, "step": 300, "token_acc": 0.5645421076852278}, {"epoch": 0.9925955573344006, "grad_norm": 1.594794750213623, "learning_rate": 7.826100901438114e-05, "loss": 1.8156473159790039, "step": 310, "token_acc": 0.5612507874692171}, {"epoch": 1.0224134480688414, "grad_norm": 1.635204792022705, "learning_rate": 7.682048209020863e-05, "loss": 1.692243766784668, "step": 320, "token_acc": 0.578695465423084}, {"epoch": 1.0544326595957574, "grad_norm": 1.597432255744934, "learning_rate": 7.534799269067953e-05, "loss": 1.6517791748046875, "step": 330, "token_acc": 0.5888049258326337}, {"epoch": 1.0864518711226736, "grad_norm": 1.6849507093429565, "learning_rate": 7.384529560923815e-05, "loss": 1.6311580657958984, "step": 340, "token_acc": 0.5899527944518911}, {"epoch": 1.1184710826495898, "grad_norm": 1.628009557723999, "learning_rate": 7.23141816383942e-05, "loss": 1.6772941589355468, "step": 350, "token_acc": 0.5857421426932783}, {"epoch": 1.1504902941765058, "grad_norm": 1.5548148155212402, "learning_rate": 7.075647543560153e-05, "loss": 1.6887575149536134, "step": 360, "token_acc": 0.5810943643512451}, {"epoch": 1.182509505703422, "grad_norm": 1.7100950479507446, "learning_rate": 6.91740333487793e-05, "loss": 1.6365116119384766, "step": 370, "token_acc": 0.591754105839416}, {"epoch": 1.2145287172303383, "grad_norm": 1.6245026588439941, "learning_rate": 6.756874120406714e-05, "loss": 1.6322790145874024, "step": 380, "token_acc": 0.5965740639068848}, {"epoch": 1.2465479287572543, "grad_norm": 1.7337063550949097, "learning_rate": 6.59425120584505e-05, "loss": 1.6207653045654298, "step": 390, "token_acc": 0.595616845960174}, {"epoch": 1.2785671402841705, "grad_norm": 1.78987717628479, "learning_rate": 6.429728391993446e-05, "loss": 1.6363605499267577, "step": 400, "token_acc": 0.5925821224628258}, {"epoch": 1.3105863518110867, "grad_norm": 1.7693870067596436, "learning_rate": 6.2635017437983e-05, "loss": 1.6822755813598633, "step": 410, "token_acc": 0.582025522671735}, {"epoch": 1.3426055633380027, "grad_norm": 1.975460171699524, "learning_rate": 6.095769356697589e-05, "loss": 1.67786865234375, "step": 420, "token_acc": 0.5874587458745875}, {"epoch": 1.374624774864919, "grad_norm": 1.7265346050262451, "learning_rate": 5.9267311205468066e-05, "loss": 1.6185420989990233, "step": 430, "token_acc": 0.59252443933295}, {"epoch": 1.4066439863918352, "grad_norm": 1.9114280939102173, "learning_rate": 5.756588481406435e-05, "loss": 1.6518224716186523, "step": 440, "token_acc": 0.5873121131741822}, {"epoch": 1.4386631979187512, "grad_norm": 1.780769944190979, "learning_rate": 5.5855442014748696e-05, "loss": 1.6496770858764649, "step": 450, "token_acc": 0.5915047196002221}, {"epoch": 1.4706824094456674, "grad_norm": 1.8532683849334717, "learning_rate": 5.413802117452874e-05, "loss": 1.6615402221679687, "step": 460, "token_acc": 0.5861300164280685}, {"epoch": 1.5027016209725836, "grad_norm": 1.8780683279037476, "learning_rate": 5.2415668976275355e-05, "loss": 1.6607166290283204, "step": 470, "token_acc": 0.5867805755395683}, {"epoch": 1.5347208324994996, "grad_norm": 1.8656740188598633, "learning_rate": 5.0690437979651974e-05, "loss": 1.618733787536621, "step": 480, "token_acc": 0.5945531914893617}, {"epoch": 1.5667400440264159, "grad_norm": 1.8098154067993164, "learning_rate": 4.8964384175040454e-05, "loss": 1.659329605102539, "step": 490, "token_acc": 0.5892032674322245}, {"epoch": 1.598759255553332, "grad_norm": 1.8880583047866821, "learning_rate": 4.723956453337843e-05, "loss": 1.6246564865112305, "step": 500, "token_acc": 0.589432703003337}, {"epoch": 1.630778467080248, "grad_norm": 1.6923528909683228, "learning_rate": 4.551803455482833e-05, "loss": 1.6054449081420898, "step": 510, "token_acc": 0.5998903809262812}, {"epoch": 1.6627976786071643, "grad_norm": 1.8087799549102783, "learning_rate": 4.380184581919879e-05, "loss": 1.6259344100952149, "step": 520, "token_acc": 0.5894066337656375}, {"epoch": 1.6948168901340805, "grad_norm": 1.7726956605911255, "learning_rate": 4.2093043541038334e-05, "loss": 1.639131736755371, "step": 530, "token_acc": 0.5925371479263695}, {"epoch": 1.7268361016609965, "grad_norm": 1.9910931587219238, "learning_rate": 4.039366413231458e-05, "loss": 1.6436588287353515, "step": 540, "token_acc": 0.5934387116015508}, {"epoch": 1.7588553131879128, "grad_norm": 1.7624704837799072, "learning_rate": 3.870573277558354e-05, "loss": 1.6382993698120116, "step": 550, "token_acc": 0.5963317820969066}, {"epoch": 1.790874524714829, "grad_norm": 1.7752238512039185, "learning_rate": 3.703126101054129e-05, "loss": 1.6159990310668946, "step": 560, "token_acc": 0.5958170085273105}, {"epoch": 1.822893736241745, "grad_norm": 1.7668604850769043, "learning_rate": 3.53722443368342e-05, "loss": 1.6044090270996094, "step": 570, "token_acc": 0.6023372287145242}, {"epoch": 1.8549129477686612, "grad_norm": 1.7946953773498535, "learning_rate": 3.373065983598417e-05, "loss": 1.6690746307373048, "step": 580, "token_acc": 0.5859519408502772}, {"epoch": 1.8869321592955774, "grad_norm": 1.9611232280731201, "learning_rate": 3.210846381526327e-05, "loss": 1.5990522384643555, "step": 590, "token_acc": 0.5986182683051136}, {"epoch": 1.9189513708224935, "grad_norm": 1.8666352033615112, "learning_rate": 3.050758947632547e-05, "loss": 1.6300153732299805, "step": 600, "token_acc": 0.5890759706069548}, {"loss": 1.472898292541504, "grad_norm": 1.829079031944275, "learning_rate": 2.8929944611373554e-05, "token_acc": 0.6195796141664267, "epoch": 1.9509705823494097, "step": 610}, {"loss": 1.4859071731567384, "grad_norm": 1.9790828227996826, "learning_rate": 2.7377409329607202e-05, "token_acc": 0.6155937301174157, "epoch": 1.982989793876326, "step": 620}, {"loss": 1.466500186920166, "grad_norm": 1.9275623559951782, "learning_rate": 2.5851833816661286e-05, "token_acc": 0.6205514683892577, "epoch": 2.0128076846107663, "step": 630}, {"loss": 1.521765899658203, "grad_norm": 1.9189571142196655, "learning_rate": 2.43550361297047e-05, "token_acc": 0.6152852650494159, "epoch": 2.0448268961376828, "step": 640}, {"loss": 1.5191805839538575, "grad_norm": 1.960351824760437, "learning_rate": 2.2888800030827253e-05, "token_acc": 0.618431328796162, "epoch": 2.0768461076645988, "step": 650}, {"loss": 1.5000973701477052, "grad_norm": 1.9569143056869507, "learning_rate": 2.145487286129671e-05, "token_acc": 0.6180468303826385, "epoch": 2.1088653191915148, "step": 660}, {"loss": 1.5209671020507813, "grad_norm": 1.8806138038635254, "learning_rate": 2.0054963459219145e-05, "token_acc": 0.6141479099678456, "epoch": 2.140884530718431, "step": 670}, {"loss": 1.506298828125, "grad_norm": 2.1668484210968018, "learning_rate": 1.8690740123084316e-05, "token_acc": 0.6213846504213163, "epoch": 2.172903742245347, "step": 680}, {"loss": 1.4665037155151368, "grad_norm": 2.4218108654022217, "learning_rate": 1.73638286236228e-05, "token_acc": 0.6214647470551414, "epoch": 2.204922953772263, "step": 690}, {"loss": 1.5111661911010743, "grad_norm": 2.0692687034606934, "learning_rate": 1.607581026634425e-05, "token_acc": 0.6147445498419039, "epoch": 2.2369421652991797, "step": 700}, {"loss": 1.4875736236572266, "grad_norm": 1.967780590057373, "learning_rate": 1.4828220007065869e-05, "token_acc": 0.6213463973138108, "epoch": 2.2689613768260957, "step": 710}, {"loss": 1.4605649948120116, "grad_norm": 2.044459342956543, "learning_rate": 1.3622544622676431e-05, "token_acc": 0.6288538873994638, "epoch": 2.3009805883530117, "step": 720}, {"loss": 1.469940757751465, "grad_norm": 2.051635265350342, "learning_rate": 1.2460220939316136e-05, "token_acc": 0.6259147403879661, "epoch": 2.332999799879928, "step": 730}, {"loss": 1.4909139633178712, "grad_norm": 2.2856085300445557, "learning_rate": 1.1342634120083951e-05, "token_acc": 0.6205475585661868, "epoch": 2.365019011406844, "step": 740}, {"loss": 1.4982398986816405, "grad_norm": 2.335191249847412, "learning_rate": 1.0271116014312293e-05, "token_acc": 0.6189350511199102, "epoch": 2.39703822293376, "step": 750}, {"loss": 1.5113862037658692, "grad_norm": 2.0195672512054443, "learning_rate": 9.246943570377232e-06, "token_acc": 0.6115606936416185, "epoch": 2.4290574344606766, "step": 760}, {"loss": 1.5082353591918944, "grad_norm": 1.9626067876815796, "learning_rate": 8.271337313934869e-06, "token_acc": 0.6186072737524668, "epoch": 2.4610766459875926, "step": 770}, {"loss": 1.468177890777588, "grad_norm": 2.056556463241577, "learning_rate": 7.345459893397882e-06, "token_acc": 0.624406855356445, "epoch": 2.4930958575145086, "step": 780}, {"loss": 1.5033246994018554, "grad_norm": 2.1522936820983887, "learning_rate": 6.470414694385418e-06, "token_acc": 0.6173043974871502, "epoch": 2.525115069041425, "step": 790}, {"loss": 1.4417503356933594, "grad_norm": 2.035142421722412, "learning_rate": 5.647244524797624e-06, "token_acc": 0.6289384252059214, "epoch": 2.557134280568341, "step": 800}, {"loss": 1.4812832832336427, "grad_norm": 2.172646999359131, "learning_rate": 4.876930372081762e-06, "token_acc": 0.6194735936693617, "epoch": 2.589153492095257, "step": 810}, {"loss": 1.503079891204834, "grad_norm": 2.028930425643921, "learning_rate": 4.16039023417088e-06, "token_acc": 0.6144811921777187, "epoch": 2.6211727036221735, "step": 820}, {"loss": 1.4801629066467286, "grad_norm": 2.358452320098877, "learning_rate": 3.4984780254884384e-06, "token_acc": 0.619050301357517, "epoch": 2.6531919151490895, "step": 830}, {"loss": 1.498103141784668, "grad_norm": 2.1185250282287598, "learning_rate": 2.8919825593222206e-06, "token_acc": 0.6221786614393727, "epoch": 2.6852111266760055, "step": 840}, {"loss": 1.4554847717285155, "grad_norm": 2.016204833984375, "learning_rate": 2.341626607780745e-06, "token_acc": 0.6240158628331487, "epoch": 2.7172303382029215, "step": 850}, {"loss": 1.4733221054077148, "grad_norm": 2.160921335220337, "learning_rate": 1.8480660404519956e-06, "token_acc": 0.6228516508367254, "epoch": 2.749249549729838, "step": 860}, {"loss": 1.4872370719909669, "grad_norm": 2.1324567794799805, "learning_rate": 1.4118890427912978e-06, "token_acc": 0.6182853887781259, "epoch": 2.781268761256754, "step": 870}, {"loss": 1.4924695014953613, "grad_norm": 2.2992053031921387, "learning_rate": 1.033615415169592e-06, "token_acc": 0.6125598422979442, "epoch": 2.8132879727836704, "step": 880}, {"loss": 1.536818504333496, "grad_norm": 2.109612226486206, "learning_rate": 7.136959534174592e-07, "token_acc": 0.6084395936892155, "epoch": 2.8453071843105864, "step": 890}, {"loss": 1.4565206527709962, "grad_norm": 2.1987407207489014, "learning_rate": 4.52511911603265e-07, "token_acc": 0.6230134216881001, "epoch": 2.8773263958375024, "step": 900}, {"loss": 1.5257400512695312, "grad_norm": 2.0174202919006348, "learning_rate": 2.503745476854613e-07, "token_acc": 0.6147926955323303, "epoch": 2.9093456073644184, "step": 910}, {"loss": 1.4633816719055175, "grad_norm": 2.092111587524414, "learning_rate": 1.0752475258059003e-07, "token_acc": 0.6237942122186495, "epoch": 2.941364818891335, "step": 920}, {"loss": 1.456690788269043, "grad_norm": 2.167464256286621, "learning_rate": 2.4132763089035338e-08, "token_acc": 0.6231332526091219, "epoch": 2.973384030418251, "step": 930}, {"train_runtime": 6879.4262, "train_samples_per_second": 2.179, "train_steps_per_second": 0.136, "total_flos": 9.702351027248886e+17, "train_loss": 0.5371509162000955, "token_acc": 0.6209577542314573, "epoch": 3.0, "step": 939}], "memory": 22.90234375}
