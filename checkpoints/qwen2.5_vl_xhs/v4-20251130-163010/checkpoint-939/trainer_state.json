{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 3.0,
  "eval_steps": 100.0,
  "global_step": 939,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.003201921152691615,
      "grad_norm": 0.9427400827407837,
      "learning_rate": 3.448275862068966e-06,
      "loss": 3.034529447555542,
      "step": 1,
      "token_acc": 0.4174095437860514
    },
    {
      "epoch": 0.03201921152691615,
      "grad_norm": 1.0394667387008667,
      "learning_rate": 3.4482758620689657e-05,
      "loss": 2.9798384772406683,
      "step": 10,
      "token_acc": 0.42581638940234134
    },
    {
      "epoch": 0.0640384230538323,
      "grad_norm": 1.0350247621536255,
      "learning_rate": 6.896551724137931e-05,
      "loss": 2.6860893249511717,
      "step": 20,
      "token_acc": 0.44986680269795387
    },
    {
      "epoch": 0.09605763458074845,
      "grad_norm": 0.9171334505081177,
      "learning_rate": 9.999970204097939e-05,
      "loss": 2.3804121017456055,
      "step": 30,
      "token_acc": 0.48846326164874554
    },
    {
      "epoch": 0.1280768461076646,
      "grad_norm": 0.8646963238716125,
      "learning_rate": 9.996395125523828e-05,
      "loss": 2.23023681640625,
      "step": 40,
      "token_acc": 0.5047477907678797
    },
    {
      "epoch": 0.16009605763458074,
      "grad_norm": 0.8422906994819641,
      "learning_rate": 9.986865748457457e-05,
      "loss": 2.0958608627319335,
      "step": 50,
      "token_acc": 0.5233090354297338
    },
    {
      "epoch": 0.1921152691614969,
      "grad_norm": 0.9719605445861816,
      "learning_rate": 9.971393429237563e-05,
      "loss": 2.091337203979492,
      "step": 60,
      "token_acc": 0.5250231696014829
    },
    {
      "epoch": 0.22413448068841305,
      "grad_norm": 1.0380438566207886,
      "learning_rate": 9.949996606519354e-05,
      "loss": 2.0668109893798827,
      "step": 70,
      "token_acc": 0.5281933899062081
    },
    {
      "epoch": 0.2561536922153292,
      "grad_norm": 1.0209375619888306,
      "learning_rate": 9.922700779300817e-05,
      "loss": 1.993912124633789,
      "step": 80,
      "token_acc": 0.5388459791004089
    },
    {
      "epoch": 0.28817290374224536,
      "grad_norm": 1.1782763004302979,
      "learning_rate": 9.889538476535084e-05,
      "loss": 2.0172470092773436,
      "step": 90,
      "token_acc": 0.5396191996351614
    },
    {
      "epoch": 0.32019211526916147,
      "grad_norm": 1.4765483140945435,
      "learning_rate": 9.850549218365053e-05,
      "loss": 2.037888526916504,
      "step": 100,
      "token_acc": 0.5312882752477452
    },
    {
      "epoch": 0.35221132679607764,
      "grad_norm": 1.3002610206604004,
      "learning_rate": 9.805779469026501e-05,
      "loss": 1.8951194763183594,
      "step": 110,
      "token_acc": 0.5468239876281574
    },
    {
      "epoch": 0.3842305383229938,
      "grad_norm": 1.3413561582565308,
      "learning_rate": 9.755282581475769e-05,
      "loss": 1.8798851013183593,
      "step": 120,
      "token_acc": 0.557489901575923
    },
    {
      "epoch": 0.4162497498499099,
      "grad_norm": 1.4786187410354614,
      "learning_rate": 9.69911873380803e-05,
      "loss": 1.901313591003418,
      "step": 130,
      "token_acc": 0.5494579945799458
    },
    {
      "epoch": 0.4482689613768261,
      "grad_norm": 1.3467682600021362,
      "learning_rate": 9.63735485754193e-05,
      "loss": 1.8468931198120118,
      "step": 140,
      "token_acc": 0.559231468849477
    },
    {
      "epoch": 0.48028817290374226,
      "grad_norm": 1.4892834424972534,
      "learning_rate": 9.570064557856017e-05,
      "loss": 1.8889873504638672,
      "step": 150,
      "token_acc": 0.5515770051317535
    },
    {
      "epoch": 0.5123073844306584,
      "grad_norm": 1.4015499353408813,
      "learning_rate": 9.49732802587207e-05,
      "loss": 1.9102113723754883,
      "step": 160,
      "token_acc": 0.5493293426449396
    },
    {
      "epoch": 0.5443265959575746,
      "grad_norm": 1.4867732524871826,
      "learning_rate": 9.419231943089822e-05,
      "loss": 1.8641101837158203,
      "step": 170,
      "token_acc": 0.5589955280357757
    },
    {
      "epoch": 0.5763458074844907,
      "grad_norm": 1.3704677820205688,
      "learning_rate": 9.335869378086972e-05,
      "loss": 1.8484542846679688,
      "step": 180,
      "token_acc": 0.5616201117318436
    },
    {
      "epoch": 0.6083650190114068,
      "grad_norm": 1.5321389436721802,
      "learning_rate": 9.247339675607605e-05,
      "loss": 1.8850658416748047,
      "step": 190,
      "token_acc": 0.5565334379729805
    },
    {
      "epoch": 0.6403842305383229,
      "grad_norm": 1.4659943580627441,
      "learning_rate": 9.153748338171192e-05,
      "loss": 1.8297218322753905,
      "step": 200,
      "token_acc": 0.5576579593659299
    },
    {
      "epoch": 0.6724034420652392,
      "grad_norm": 1.466857671737671,
      "learning_rate": 9.055206900343228e-05,
      "loss": 1.797056007385254,
      "step": 210,
      "token_acc": 0.5657085337210603
    },
    {
      "epoch": 0.7044226535921553,
      "grad_norm": 1.484342336654663,
      "learning_rate": 8.951832795817397e-05,
      "loss": 1.863274383544922,
      "step": 220,
      "token_acc": 0.5533819058000115
    },
    {
      "epoch": 0.7364418651190714,
      "grad_norm": 1.7169773578643799,
      "learning_rate": 8.843749217467612e-05,
      "loss": 1.8710081100463867,
      "step": 230,
      "token_acc": 0.5552533718155076
    },
    {
      "epoch": 0.7684610766459876,
      "grad_norm": 1.7191188335418701,
      "learning_rate": 8.731084970536738e-05,
      "loss": 1.864406204223633,
      "step": 240,
      "token_acc": 0.555311194193974
    },
    {
      "epoch": 0.8004802881729037,
      "grad_norm": 1.534696340560913,
      "learning_rate": 8.613974319136958e-05,
      "loss": 1.7848968505859375,
      "step": 250,
      "token_acc": 0.5657955330416763
    },
    {
      "epoch": 0.8324994996998198,
      "grad_norm": 1.367798924446106,
      "learning_rate": 8.492556826244687e-05,
      "loss": 1.7934467315673828,
      "step": 260,
      "token_acc": 0.5648309613545592
    },
    {
      "epoch": 0.8645187112267361,
      "grad_norm": 1.6910395622253418,
      "learning_rate": 8.36697718738073e-05,
      "loss": 1.8091894149780274,
      "step": 270,
      "token_acc": 0.5669140800719749
    },
    {
      "epoch": 0.8965379227536522,
      "grad_norm": 1.4645434617996216,
      "learning_rate": 8.237385058173912e-05,
      "loss": 1.8422256469726563,
      "step": 280,
      "token_acc": 0.5582601755786113
    },
    {
      "epoch": 0.9285571342805683,
      "grad_norm": 1.568582534790039,
      "learning_rate": 8.103934876013623e-05,
      "loss": 1.7899974822998046,
      "step": 290,
      "token_acc": 0.5666529096161783
    },
    {
      "epoch": 0.9605763458074845,
      "grad_norm": 1.4879812002182007,
      "learning_rate": 7.966785676003866e-05,
      "loss": 1.7816152572631836,
      "step": 300,
      "token_acc": 0.5645421076852278
    },
    {
      "epoch": 0.9925955573344006,
      "grad_norm": 1.594794750213623,
      "learning_rate": 7.826100901438114e-05,
      "loss": 1.8156473159790039,
      "step": 310,
      "token_acc": 0.5612507874692171
    },
    {
      "epoch": 1.0224134480688414,
      "grad_norm": 1.635204792022705,
      "learning_rate": 7.682048209020863e-05,
      "loss": 1.692243766784668,
      "step": 320,
      "token_acc": 0.578695465423084
    },
    {
      "epoch": 1.0544326595957574,
      "grad_norm": 1.597432255744934,
      "learning_rate": 7.534799269067953e-05,
      "loss": 1.6517791748046875,
      "step": 330,
      "token_acc": 0.5888049258326337
    },
    {
      "epoch": 1.0864518711226736,
      "grad_norm": 1.6849507093429565,
      "learning_rate": 7.384529560923815e-05,
      "loss": 1.6311580657958984,
      "step": 340,
      "token_acc": 0.5899527944518911
    },
    {
      "epoch": 1.1184710826495898,
      "grad_norm": 1.628009557723999,
      "learning_rate": 7.23141816383942e-05,
      "loss": 1.6772941589355468,
      "step": 350,
      "token_acc": 0.5857421426932783
    },
    {
      "epoch": 1.1504902941765058,
      "grad_norm": 1.5548148155212402,
      "learning_rate": 7.075647543560153e-05,
      "loss": 1.6887575149536134,
      "step": 360,
      "token_acc": 0.5810943643512451
    },
    {
      "epoch": 1.182509505703422,
      "grad_norm": 1.7100950479507446,
      "learning_rate": 6.91740333487793e-05,
      "loss": 1.6365116119384766,
      "step": 370,
      "token_acc": 0.591754105839416
    },
    {
      "epoch": 1.2145287172303383,
      "grad_norm": 1.6245026588439941,
      "learning_rate": 6.756874120406714e-05,
      "loss": 1.6322790145874024,
      "step": 380,
      "token_acc": 0.5965740639068848
    },
    {
      "epoch": 1.2465479287572543,
      "grad_norm": 1.7337063550949097,
      "learning_rate": 6.59425120584505e-05,
      "loss": 1.6207653045654298,
      "step": 390,
      "token_acc": 0.595616845960174
    },
    {
      "epoch": 1.2785671402841705,
      "grad_norm": 1.78987717628479,
      "learning_rate": 6.429728391993446e-05,
      "loss": 1.6363605499267577,
      "step": 400,
      "token_acc": 0.5925821224628258
    },
    {
      "epoch": 1.3105863518110867,
      "grad_norm": 1.7693870067596436,
      "learning_rate": 6.2635017437983e-05,
      "loss": 1.6822755813598633,
      "step": 410,
      "token_acc": 0.582025522671735
    },
    {
      "epoch": 1.3426055633380027,
      "grad_norm": 1.975460171699524,
      "learning_rate": 6.095769356697589e-05,
      "loss": 1.67786865234375,
      "step": 420,
      "token_acc": 0.5874587458745875
    },
    {
      "epoch": 1.374624774864919,
      "grad_norm": 1.7265346050262451,
      "learning_rate": 5.9267311205468066e-05,
      "loss": 1.6185420989990233,
      "step": 430,
      "token_acc": 0.59252443933295
    },
    {
      "epoch": 1.4066439863918352,
      "grad_norm": 1.9114280939102173,
      "learning_rate": 5.756588481406435e-05,
      "loss": 1.6518224716186523,
      "step": 440,
      "token_acc": 0.5873121131741822
    },
    {
      "epoch": 1.4386631979187512,
      "grad_norm": 1.780769944190979,
      "learning_rate": 5.5855442014748696e-05,
      "loss": 1.6496770858764649,
      "step": 450,
      "token_acc": 0.5915047196002221
    },
    {
      "epoch": 1.4706824094456674,
      "grad_norm": 1.8532683849334717,
      "learning_rate": 5.413802117452874e-05,
      "loss": 1.6615402221679687,
      "step": 460,
      "token_acc": 0.5861300164280685
    },
    {
      "epoch": 1.5027016209725836,
      "grad_norm": 1.8780683279037476,
      "learning_rate": 5.2415668976275355e-05,
      "loss": 1.6607166290283204,
      "step": 470,
      "token_acc": 0.5867805755395683
    },
    {
      "epoch": 1.5347208324994996,
      "grad_norm": 1.8656740188598633,
      "learning_rate": 5.0690437979651974e-05,
      "loss": 1.618733787536621,
      "step": 480,
      "token_acc": 0.5945531914893617
    },
    {
      "epoch": 1.5667400440264159,
      "grad_norm": 1.8098154067993164,
      "learning_rate": 4.8964384175040454e-05,
      "loss": 1.659329605102539,
      "step": 490,
      "token_acc": 0.5892032674322245
    },
    {
      "epoch": 1.598759255553332,
      "grad_norm": 1.8880583047866821,
      "learning_rate": 4.723956453337843e-05,
      "loss": 1.6246564865112305,
      "step": 500,
      "token_acc": 0.589432703003337
    },
    {
      "epoch": 1.630778467080248,
      "grad_norm": 1.6923528909683228,
      "learning_rate": 4.551803455482833e-05,
      "loss": 1.6054449081420898,
      "step": 510,
      "token_acc": 0.5998903809262812
    },
    {
      "epoch": 1.6627976786071643,
      "grad_norm": 1.8087799549102783,
      "learning_rate": 4.380184581919879e-05,
      "loss": 1.6259344100952149,
      "step": 520,
      "token_acc": 0.5894066337656375
    },
    {
      "epoch": 1.6948168901340805,
      "grad_norm": 1.7726956605911255,
      "learning_rate": 4.2093043541038334e-05,
      "loss": 1.639131736755371,
      "step": 530,
      "token_acc": 0.5925371479263695
    },
    {
      "epoch": 1.7268361016609965,
      "grad_norm": 1.9910931587219238,
      "learning_rate": 4.039366413231458e-05,
      "loss": 1.6436588287353515,
      "step": 540,
      "token_acc": 0.5934387116015508
    },
    {
      "epoch": 1.7588553131879128,
      "grad_norm": 1.7624704837799072,
      "learning_rate": 3.870573277558354e-05,
      "loss": 1.6382993698120116,
      "step": 550,
      "token_acc": 0.5963317820969066
    },
    {
      "epoch": 1.790874524714829,
      "grad_norm": 1.7752238512039185,
      "learning_rate": 3.703126101054129e-05,
      "loss": 1.6159990310668946,
      "step": 560,
      "token_acc": 0.5958170085273105
    },
    {
      "epoch": 1.822893736241745,
      "grad_norm": 1.7668604850769043,
      "learning_rate": 3.53722443368342e-05,
      "loss": 1.6044090270996094,
      "step": 570,
      "token_acc": 0.6023372287145242
    },
    {
      "epoch": 1.8549129477686612,
      "grad_norm": 1.7946953773498535,
      "learning_rate": 3.373065983598417e-05,
      "loss": 1.6690746307373048,
      "step": 580,
      "token_acc": 0.5859519408502772
    },
    {
      "epoch": 1.8869321592955774,
      "grad_norm": 1.9611232280731201,
      "learning_rate": 3.210846381526327e-05,
      "loss": 1.5990522384643555,
      "step": 590,
      "token_acc": 0.5986182683051136
    },
    {
      "epoch": 1.9189513708224935,
      "grad_norm": 1.8666352033615112,
      "learning_rate": 3.050758947632547e-05,
      "loss": 1.6300153732299805,
      "step": 600,
      "token_acc": 0.5890759706069548
    },
    {
      "epoch": 1.9509705823494097,
      "grad_norm": 1.829079031944275,
      "learning_rate": 2.8929944611373554e-05,
      "loss": 1.472898292541504,
      "step": 610,
      "token_acc": 0.6195796141664267
    },
    {
      "epoch": 1.982989793876326,
      "grad_norm": 1.9790828227996826,
      "learning_rate": 2.7377409329607202e-05,
      "loss": 1.4859071731567384,
      "step": 620,
      "token_acc": 0.6155937301174157
    },
    {
      "epoch": 2.0128076846107663,
      "grad_norm": 1.9275623559951782,
      "learning_rate": 2.5851833816661286e-05,
      "loss": 1.466500186920166,
      "step": 630,
      "token_acc": 0.6205514683892577
    },
    {
      "epoch": 2.0448268961376828,
      "grad_norm": 1.9189571142196655,
      "learning_rate": 2.43550361297047e-05,
      "loss": 1.521765899658203,
      "step": 640,
      "token_acc": 0.6152852650494159
    },
    {
      "epoch": 2.0768461076645988,
      "grad_norm": 1.960351824760437,
      "learning_rate": 2.2888800030827253e-05,
      "loss": 1.5191805839538575,
      "step": 650,
      "token_acc": 0.618431328796162
    },
    {
      "epoch": 2.1088653191915148,
      "grad_norm": 1.9569143056869507,
      "learning_rate": 2.145487286129671e-05,
      "loss": 1.5000973701477052,
      "step": 660,
      "token_acc": 0.6180468303826385
    },
    {
      "epoch": 2.140884530718431,
      "grad_norm": 1.8806138038635254,
      "learning_rate": 2.0054963459219145e-05,
      "loss": 1.5209671020507813,
      "step": 670,
      "token_acc": 0.6141479099678456
    },
    {
      "epoch": 2.172903742245347,
      "grad_norm": 2.1668484210968018,
      "learning_rate": 1.8690740123084316e-05,
      "loss": 1.506298828125,
      "step": 680,
      "token_acc": 0.6213846504213163
    },
    {
      "epoch": 2.204922953772263,
      "grad_norm": 2.4218108654022217,
      "learning_rate": 1.73638286236228e-05,
      "loss": 1.4665037155151368,
      "step": 690,
      "token_acc": 0.6214647470551414
    },
    {
      "epoch": 2.2369421652991797,
      "grad_norm": 2.0692687034606934,
      "learning_rate": 1.607581026634425e-05,
      "loss": 1.5111661911010743,
      "step": 700,
      "token_acc": 0.6147445498419039
    },
    {
      "epoch": 2.2689613768260957,
      "grad_norm": 1.967780590057373,
      "learning_rate": 1.4828220007065869e-05,
      "loss": 1.4875736236572266,
      "step": 710,
      "token_acc": 0.6213463973138108
    },
    {
      "epoch": 2.3009805883530117,
      "grad_norm": 2.044459342956543,
      "learning_rate": 1.3622544622676431e-05,
      "loss": 1.4605649948120116,
      "step": 720,
      "token_acc": 0.6288538873994638
    },
    {
      "epoch": 2.332999799879928,
      "grad_norm": 2.051635265350342,
      "learning_rate": 1.2460220939316136e-05,
      "loss": 1.469940757751465,
      "step": 730,
      "token_acc": 0.6259147403879661
    },
    {
      "epoch": 2.365019011406844,
      "grad_norm": 2.2856085300445557,
      "learning_rate": 1.1342634120083951e-05,
      "loss": 1.4909139633178712,
      "step": 740,
      "token_acc": 0.6205475585661868
    },
    {
      "epoch": 2.39703822293376,
      "grad_norm": 2.335191249847412,
      "learning_rate": 1.0271116014312293e-05,
      "loss": 1.4982398986816405,
      "step": 750,
      "token_acc": 0.6189350511199102
    },
    {
      "epoch": 2.4290574344606766,
      "grad_norm": 2.0195672512054443,
      "learning_rate": 9.246943570377232e-06,
      "loss": 1.5113862037658692,
      "step": 760,
      "token_acc": 0.6115606936416185
    },
    {
      "epoch": 2.4610766459875926,
      "grad_norm": 1.9626067876815796,
      "learning_rate": 8.271337313934869e-06,
      "loss": 1.5082353591918944,
      "step": 770,
      "token_acc": 0.6186072737524668
    },
    {
      "epoch": 2.4930958575145086,
      "grad_norm": 2.056556463241577,
      "learning_rate": 7.345459893397882e-06,
      "loss": 1.468177890777588,
      "step": 780,
      "token_acc": 0.624406855356445
    },
    {
      "epoch": 2.525115069041425,
      "grad_norm": 2.1522936820983887,
      "learning_rate": 6.470414694385418e-06,
      "loss": 1.5033246994018554,
      "step": 790,
      "token_acc": 0.6173043974871502
    },
    {
      "epoch": 2.557134280568341,
      "grad_norm": 2.035142421722412,
      "learning_rate": 5.647244524797624e-06,
      "loss": 1.4417503356933594,
      "step": 800,
      "token_acc": 0.6289384252059214
    },
    {
      "epoch": 2.589153492095257,
      "grad_norm": 2.172646999359131,
      "learning_rate": 4.876930372081762e-06,
      "loss": 1.4812832832336427,
      "step": 810,
      "token_acc": 0.6194735936693617
    },
    {
      "epoch": 2.6211727036221735,
      "grad_norm": 2.028930425643921,
      "learning_rate": 4.16039023417088e-06,
      "loss": 1.503079891204834,
      "step": 820,
      "token_acc": 0.6144811921777187
    },
    {
      "epoch": 2.6531919151490895,
      "grad_norm": 2.358452320098877,
      "learning_rate": 3.4984780254884384e-06,
      "loss": 1.4801629066467286,
      "step": 830,
      "token_acc": 0.619050301357517
    },
    {
      "epoch": 2.6852111266760055,
      "grad_norm": 2.1185250282287598,
      "learning_rate": 2.8919825593222206e-06,
      "loss": 1.498103141784668,
      "step": 840,
      "token_acc": 0.6221786614393727
    },
    {
      "epoch": 2.7172303382029215,
      "grad_norm": 2.016204833984375,
      "learning_rate": 2.341626607780745e-06,
      "loss": 1.4554847717285155,
      "step": 850,
      "token_acc": 0.6240158628331487
    },
    {
      "epoch": 2.749249549729838,
      "grad_norm": 2.160921335220337,
      "learning_rate": 1.8480660404519956e-06,
      "loss": 1.4733221054077148,
      "step": 860,
      "token_acc": 0.6228516508367254
    },
    {
      "epoch": 2.781268761256754,
      "grad_norm": 2.1324567794799805,
      "learning_rate": 1.4118890427912978e-06,
      "loss": 1.4872370719909669,
      "step": 870,
      "token_acc": 0.6182853887781259
    },
    {
      "epoch": 2.8132879727836704,
      "grad_norm": 2.2992053031921387,
      "learning_rate": 1.033615415169592e-06,
      "loss": 1.4924695014953613,
      "step": 880,
      "token_acc": 0.6125598422979442
    },
    {
      "epoch": 2.8453071843105864,
      "grad_norm": 2.109612226486206,
      "learning_rate": 7.136959534174592e-07,
      "loss": 1.536818504333496,
      "step": 890,
      "token_acc": 0.6084395936892155
    },
    {
      "epoch": 2.8773263958375024,
      "grad_norm": 2.1987407207489014,
      "learning_rate": 4.52511911603265e-07,
      "loss": 1.4565206527709962,
      "step": 900,
      "token_acc": 0.6230134216881001
    },
    {
      "epoch": 2.9093456073644184,
      "grad_norm": 2.0174202919006348,
      "learning_rate": 2.503745476854613e-07,
      "loss": 1.5257400512695312,
      "step": 910,
      "token_acc": 0.6147926955323303
    },
    {
      "epoch": 2.941364818891335,
      "grad_norm": 2.092111587524414,
      "learning_rate": 1.0752475258059003e-07,
      "loss": 1.4633816719055175,
      "step": 920,
      "token_acc": 0.6237942122186495
    },
    {
      "epoch": 2.973384030418251,
      "grad_norm": 2.167464256286621,
      "learning_rate": 2.4132763089035338e-08,
      "loss": 1.456690788269043,
      "step": 930,
      "token_acc": 0.6231332526091219
    }
  ],
  "logging_steps": 10,
  "max_steps": 939,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 100,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 9.702351027248886e+17,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
